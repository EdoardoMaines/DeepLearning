{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matea/Documents/VisualGrounding/DeepLearning/DeepBlasphemy/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "\n",
    "import pickle # Load refs and annotations\n",
    "from typing import Any, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torchvision\n",
    "import torchmetrics\n",
    "\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration \n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import clip\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import sklearn.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Sentence Tensor:\n",
      "tensor([[-0.9051, -0.4523, -0.6808,  ..., -0.5678, -0.7033,  0.9266],\n",
      "        [-0.8462, -0.2268, -0.4501,  ..., -0.0893, -0.6163,  0.8899],\n",
      "        [-0.8705, -0.4604, -0.8941,  ..., -0.8175, -0.6987,  0.8999]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Example sentences as a list of strings\n",
    "sentences = [\n",
    "    \"This is the first sentence.\",\n",
    "    \"And this is the second one.\",\n",
    "    \"Finally, here's the third sentence.\"\n",
    "]\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and encode sentences using BERT\n",
    "encoded_inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_inputs)\n",
    "\n",
    "\n",
    "\n",
    "# Get the tensor containing the sentence representations (pooled outputs)\n",
    "sentence_tensor = outputs.pooler_output\n",
    "\n",
    "print(\"Encoded Sentence Tensor:\")\n",
    "print(sentence_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "import json\n",
    "\n",
    "    \n",
    "def getcaption(elem):\n",
    "    li = []\n",
    "    for e in elem[\"sentences\"]:\n",
    "        li.append(e['raw'])\n",
    "    return li\n",
    "\n",
    "class RefCOCOg(Dataset):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataset: a list of dictionaries containing:\n",
    "        {\n",
    "            'file_name': # path of the image, images will be loaded on the fly\n",
    "            'caption': # referring caption\n",
    "            'ann_id': # annotation ID (one per caption), taken from 'file_name'\n",
    "            'bbox': # coordinates (xmin, ymin, xmax, ymax) of the bounding box\n",
    "        }\n",
    "    \"\"\"\n",
    "    def __init__(self, refs, annotations, split=\"train\"):\n",
    "\n",
    "        self.dataset = [{\"file_name\": os.path.join(\"../refcocog/images/\", f'{\"_\".join(elem[\"file_name\"].split(\"_\")[:3])}.jpg'),\n",
    "                            \"caption\": elem[\"sentences\"][0][\"raw\"],\n",
    "                            \"captions\": getcaption(elem),\n",
    "                            \"ann_id\": int(elem[\"file_name\"].split(\"_\")[3][:-4]),\n",
    "                            \"bbox\": annotations[int(elem[\"file_name\"].split(\"_\")[3][:-4])]}\n",
    "                        for elem in [d for d in refs if d[\"split\"]==split]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    def __call__(self, idx):\n",
    "        print(json.dumps(self.dataset[idx], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load refs and annotations\n",
    "import pickle\n",
    "\n",
    "with open(\"../refcocog/annotations/refs(umd).p\", \"rb\") as fp:\n",
    "  refs = pickle.load(fp)\n",
    "\n",
    "# 'annotations' will be a dict object mapping the 'annotation_id' to the 'bbox' to make search faster\n",
    "with open(\"../refcocog/annotations/instances.json\", \"rb\") as fp:\n",
    "  data = json.load(fp)\n",
    "  annotations = dict(sorted({ann[\"id\"]: ann[\"bbox\"] for ann in data[\"annotations\"]}.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_name': '../refcocog/images/COCO_train2014_000000380440.jpg', 'caption': 'the man in yellow coat', 'captions': ['the man in yellow coat', 'Skiier in red pants.'], 'ann_id': 491042, 'bbox': [374.31, 65.06, 136.04, 201.94]}\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def pad_image(image):\n",
    "    \"\"\"\n",
    "    Performs bottom-right padding of the original image to 640x640 (max size of images in the dataset).\n",
    "    Bottom-right padding prevents corruption of bounding boxes.\n",
    "\n",
    "    ### Arguments\n",
    "    image: a PIL.Image to transform\n",
    "    \"\"\"\n",
    "    original_width, original_height = image.size\n",
    "    padded_width, padded_height = 640, 640\n",
    "\n",
    "    pad_width = padded_width - original_width\n",
    "    pad_height = padded_height - original_height\n",
    "\n",
    "    padded_image = Image.new(image.mode, (padded_width, padded_height), (0, 0, 0))\n",
    "    padded_image.paste(image, (0, 0))\n",
    "\n",
    "    return padded_image\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    for sample in batch:\n",
    "        image = Image.open(sample[\"file_name\"]).convert(\"RGB\")\n",
    "        image = pad_image(image=image)\n",
    "        images.append(transform(image))\n",
    "    images = torch.stack(images, dim=0)\n",
    "\n",
    "    data = {}\n",
    "    for key in batch[0].keys():\n",
    "        if key != \"file_name\":\n",
    "            data[key] = [sample[key] for sample in batch]\n",
    "    return images, data\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# create dataset and dataloader\n",
    "dataset = RefCOCOg(refs, annotations, split=\"test\")\n",
    "print(dataset[0])\n",
    "print(\"---------------------------------------------------\")\n",
    "#plt.imshow(Image.open(dataset[2][\"file_name\"]))\n",
    "dataloader = DataLoader(dataset, batch_size=1, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the man in yellow coat', 'Skiier in red pants.']\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0]['captions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Sentence Tensor:\n",
      "tensor([[-0.8468, -0.2413, -0.3398,  ...,  0.1472, -0.6613,  0.9029],\n",
      "        [-0.7858, -0.3866, -0.8730,  ..., -0.8093, -0.6661,  0.9216]])\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = tokenizer(dataset[0]['captions'], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_inputs)\n",
    "\n",
    "# Get the tensor containing the sentence representations (pooled outputs)\n",
    "sentence_tensor = outputs.pooler_output\n",
    "\n",
    "print(\"Encoded Sentence Tensor:\")\n",
    "print(sentence_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened Tensor:\n",
      "tensor([-0.8468, -0.2413, -0.3398,  ..., -0.8093, -0.6661,  0.9216])\n"
     ]
    }
   ],
   "source": [
    "sentence_tensor = outputs.pooler_output\n",
    "\n",
    "# Flatten the tensor to get a single tensor with dim 1\n",
    "flattened_tensor = sentence_tensor.view(-1)\n",
    "print(\"Flattened Tensor:\")\n",
    "print(flattened_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device =  \"cpu\"\n",
    "model, preprocess = clip.load(\"RN50\", device=device)\n",
    "#text = clip.tokenize(q).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/matea/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 🚀 2023-8-28 Python-3.11.3 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 5938MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " CAPTION:  the man in yellow coat\n",
      "RESULTS:  [[     376.61      67.176      511.16      262.77     0.93054           0]\n",
      " [     230.67       43.14       371.3      307.89     0.93043           0]\n",
      " [     244.57      257.03       377.8      343.93     0.70337          30]\n",
      " [     346.77      212.96      518.62      268.03     0.44683          30]]\n"
     ]
    }
   ],
   "source": [
    "# Models\n",
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# Images\n",
    "img = dataset[0]['file_name']\n",
    "# Query\n",
    "q = dataset[0]['caption']\n",
    "\n",
    "# Inference\n",
    "results = yolo_model(img)\n",
    "\n",
    "# Results\n",
    "print(\"\\n CAPTION: \", q)\n",
    "print(\"RESULTS: \", results.xyxy[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_features_transposed = sentence_tensor.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     376.61      67.176      511.16      262.77     0.93054           0]\n",
      "1024\n",
      "2\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [2, 1024] but got: [2, 768].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m image_features_expanded \u001b[39m=\u001b[39m image_features\u001b[39m.\u001b[39mexpand(sentence_tensor\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \n\u001b[1;32m     22\u001b[0m \u001b[39m#logits_per_image, logits_per_text = model(image, flattened_tensor)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m similarity_score \u001b[39m=\u001b[39m (\u001b[39m100.0\u001b[39;49m \u001b[39m*\u001b[39;49m image_features_expanded \u001b[39m@\u001b[39;49m bert_features_expanded\u001b[39m.\u001b[39;49mT)\u001b[39m.\u001b[39msoftmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[39m#similarity_scores = cosine_similarity(bert_features_expanded.T, image_features)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m prob \u001b[39m=\u001b[39m similarity_score\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [2, 1024] but got: [2, 768]."
     ]
    }
   ],
   "source": [
    "best_bbox = []\n",
    "max_prob = 0\n",
    "for bbox in results.xyxy[0].cpu().numpy():\n",
    "    print(bbox)\n",
    "    temp = cv2.imread(dataset[0]['file_name'])\n",
    "    #print(temp)\n",
    "    image = np.zeros((temp.shape[0], temp.shape[1], temp.shape[2]), dtype=np.uint8)\n",
    "    image[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])] = temp[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]\n",
    "    image = preprocess(Image.fromarray(image)).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        #text_features = model.encode_text(flattened_tensor)\n",
    "        \n",
    "        print(image_features.shape[1])\n",
    "        print(sentence_tensor.shape[0])\n",
    "\n",
    "        num_rows_image = image_features.shape[0]\n",
    "        bert_features_expanded = bert_features_transposed.expand(num_rows_image, -1, -1)\n",
    "        image_features_expanded = image_features.expand(sentence_tensor.shape[0], -1) \n",
    "\n",
    "        #logits_per_image, logits_per_text = model(image, flattened_tensor)\n",
    "        similarity_score = (100.0 * image_features_expanded @ bert_features_expanded.T).softmax(dim=1)\n",
    "        #similarity_scores = cosine_similarity(bert_features_expanded.T, image_features)\n",
    "\n",
    "        prob = similarity_score.cpu().numpy()[0][0]\n",
    "        if prob > max_prob:\n",
    "            #print(\"ciao\")\n",
    "            max_prob = prob\n",
    "            best_bbox = bbox\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepBlasphemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 1. Prep Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "\n",
    "import pickle # Load refs and annotations\n",
    "from typing import Any, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torchvision\n",
    "import torchmetrics\n",
    "\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration \n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "import clip\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "import json\n",
    "\n",
    "class RefCOCOg(Dataset):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataset: a list of dictionaries containing:\n",
    "        {\n",
    "            'file_name': # path of the image, images will be loaded on the fly\n",
    "            'caption': # referring caption\n",
    "            'ann_id': # annotation ID (one per caption), taken from 'file_name'\n",
    "            'bbox': # coordinates (xmin, ymin, xmax, ymax) of the bounding box\n",
    "        }\n",
    "    \"\"\"\n",
    "    def __init__(self, refs, annotations, split=\"train\"):\n",
    "\n",
    "        self.dataset = [{\"file_name\": os.path.join(\"./refcocog/images/\", f'{\"_\".join(elem[\"file_name\"].split(\"_\")[:3])}.jpg'),\n",
    "                            \"caption\": elem[\"sentences\"][0][\"raw\"],\n",
    "                            \"ann_id\": int(elem[\"file_name\"].split(\"_\")[3][:-4]),\n",
    "                            \"bbox\": annotations[int(elem[\"file_name\"].split(\"_\")[3][:-4])]}\n",
    "                        for elem in [d for d in refs if d[\"split\"]==split]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    def __call__(self, idx):\n",
    "        print(json.dumps(self.dataset[idx], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m tokenizer \u001b[39m=\u001b[39m SimpleTokenizer()\n\u001b[0;32m---> 10\u001b[0m model, preprocess \u001b[39m=\u001b[39m clip\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mRN50\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     11\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mcuda()\u001b[39m.\u001b[39meval()\n\u001b[1;32m     13\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mRefCOCOGDataset\u001b[39;00m(Dataset):\n",
      "File \u001b[0;32m~/Documents/VisualGrounding/DeepLearning/DeepBlasphemy/lib/python3.11/site-packages/clip/clip.py:139\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, device, jit, download_root)\u001b[0m\n\u001b[1;32m    136\u001b[0m         state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(opened_file, map_location\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m jit:\n\u001b[0;32m--> 139\u001b[0m     model \u001b[39m=\u001b[39m build_model(state_dict \u001b[39mor\u001b[39;49;00m model\u001b[39m.\u001b[39;49mstate_dict())\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m    140\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mstr\u001b[39m(device) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    141\u001b[0m         model\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/Documents/VisualGrounding/DeepLearning/DeepBlasphemy/lib/python3.11/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/Documents/VisualGrounding/DeepLearning/DeepBlasphemy/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/VisualGrounding/DeepLearning/DeepBlasphemy/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/VisualGrounding/DeepLearning/DeepBlasphemy/lib/python3.11/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/Documents/VisualGrounding/DeepLearning/DeepBlasphemy/lib/python3.11/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from clip.simple_tokenizer import SimpleTokenizer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = SimpleTokenizer()\n",
    "model, preprocess = clip.load(\"RN50\")\n",
    "model = model.cuda().eval()\n",
    "\n",
    "class RefCOCOGDataset(Dataset):\n",
    "    def __init__(self, root_dir, split_type, transform):\n",
    "        self.root_dir = root_dir\n",
    "        self.split_type = split_type\n",
    "        self.transform = transform\n",
    "        self.annotations = self._load_annotations()\n",
    "\n",
    "    def _load_annotations(self):\n",
    "        annotations_file = os.path.join(self.root_dir + '/annotations/', 'refs(umd).p')\n",
    "        with open(annotations_file, 'rb') as f:\n",
    "            annotations = pickle.load(f, encoding='latin1')\n",
    "        return annotations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation = self.annotations[idx]\n",
    "        image_id = annotation['image_id']\n",
    "        image_path = os.path.join(self.root_dir, 'images', f'COCO_train2014_{str(image_id).zfill(12)}.jpg')\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        # Assuming you want the first sentence as the target\n",
    "        target = annotation['sentences'][0]['raw']\n",
    "        captions = [sent['raw'] for sent in annotation['sentences']]\n",
    "        caption_tokens = [tokenizer.encode(caption) for caption in captions]\n",
    "\n",
    "        # You might need to preprocess the target, for example, convert words to indices.\n",
    "\n",
    "        sample = {'image': image, 'target': caption_tokens}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obj = pickle.loads(s)\n",
    "pprint.pprint(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./refcocog/annotations/refs(umd).p\", \"rb\") as fp:\n",
    "  refs = pickle.load(fp)\n",
    "\n",
    "# 'annotations' will be a dict object mapping the 'annotation_id' to the 'bbox' to make search faster\n",
    "with open(\"./refcocog/annotations/instances.json\", \"rb\") as fp:\n",
    "  data = json.load(fp)\n",
    "  annotations = dict(sorted({ann[\"id\"]: ann[\"bbox\"] for ann in data[\"annotations\"]}.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m dataset_root \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./refcocog\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m test \u001b[39m=\u001b[39m RefCOCOGDataset(root_dir\u001b[39m=\u001b[39mdataset_root, split_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m,transform \u001b[39m=\u001b[39m transform2)\n\u001b[0;32m---> 59\u001b[0m display(test[\u001b[39m2\u001b[39;49m][\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     60\u001b[0m \u001b[39mprint\u001b[39m(test[\u001b[39m0\u001b[39m])\n\u001b[1;32m     61\u001b[0m dataloader_test \u001b[39m=\u001b[39m DataLoader(test, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[71], line 31\u001b[0m, in \u001b[0;36mRefCOCOGDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39m# Assuming you want the first sentence as the target\u001b[39;00m\n\u001b[1;32m     30\u001b[0m target \u001b[39m=\u001b[39m annotation[\u001b[39m'\u001b[39m\u001b[39msentences\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 31\u001b[0m taeget \u001b[39m=\u001b[39m clip\u001b[39m.\u001b[39;49mtokenize(target)\u001b[39m.\u001b[39;49mcuda()\n\u001b[1;32m     33\u001b[0m \u001b[39m# You might need to preprocess the target, for example, convert words to indices.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m sample \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m: image, \u001b[39m'\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m'\u001b[39m: target}\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "def pad_image(image):\n",
    "    \"\"\"\n",
    "    Performs bottom-right padding of the original image to 640x640 (max size of images in the dataset).\n",
    "    Bottom-right padding prevents corruption of bounding boxes.\n",
    "\n",
    "    ### Arguments\n",
    "    image: a PIL.Image to transform\n",
    "    \"\"\"\n",
    "    original_width, original_height = image.size\n",
    "    padded_width, padded_height = 640, 640\n",
    "\n",
    "    #pad_width = padded_width - original_width\n",
    "    #pad_height = padded_height - original_height\n",
    "\n",
    "    padded_image = Image.new(image.mode, (padded_width, padded_height), (0, 0, 0))\n",
    "    padded_image.paste(image, (0, 0))\n",
    "\n",
    "    return padded_image\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    for sample in batch:\n",
    "        image = Image.open(sample[\"file_name\"]).convert(\"RGB\")\n",
    "        image = pad_image(image=image)\n",
    "        images.append(transform(image))\n",
    "    images = torch.stack(images, dim=0) #Concatenates a sequence of tensors along a new dimension.\n",
    "\n",
    "    data = {}\n",
    "    for key in batch[0].keys():\n",
    "        if key != \"file_name\":\n",
    "            data[key] = [sample[key] for sample in batch]\n",
    "    \n",
    "    print(data)\n",
    "    print(\"--------------------------\")\n",
    "\n",
    "    return images, data\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\"\"\"\n",
    "# create dataset and dataloader\n",
    "dataset_train = RefCOCOg(refs, annotations, split=\"train\")\n",
    "dataset_test = RefCOCOg(refs, annotations, split=\"test\")\n",
    "plt.imshow(Image.open(dataset_train[2][\"file_name\"]))\n",
    "plt.imshow(Image.open(dataset_test[2][\"file_name\"]))\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=1, collate_fn=collate_fn)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=1, collate_fn=collate_fn)\n",
    "\"\"\"\n",
    "transform2 = transforms.Compose([\n",
    "    transforms.Pad(0, fill=0),  # Replace padding_value with your desired padding\n",
    "    transforms.Resize((224, 224)),  # ResNet-50 input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Pre-trained ResNet-50 normalization\n",
    "])\n",
    "\n",
    "dataset_root = \"./refcocog\"\n",
    "test = RefCOCOGDataset(root_dir=dataset_root, split_type='test',transform = transform2)\n",
    "display(test[2]['image'])\n",
    "print(test[0])\n",
    "dataloader_test = DataLoader(test, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_plt(path):\n",
    "    \"\"\" Load image with matplotlib\"\"\"\n",
    "    return plt.imread(path)\n",
    "\n",
    "def load_image_pil(path):\n",
    "    \"\"\" Load image with PIL\"\"\"\n",
    "    return Image.open(path)\n",
    "\n",
    "def get_distance_box_iou_accuracy(box_pred, box_true, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Given the target boxes and the prediction return the\n",
    "    accuracy of the prediction. The accuracy is computed as\n",
    "    the percentage of boxes that have an IoU > iou_threshold\n",
    "    with the target box.\n",
    "\n",
    "    Args:\n",
    "    @params box_pred: tensor of shape (batch_size, n_boxes, 4)\n",
    "    @params box_true: tensor of shape (batch_size, n_boxes, 4)\n",
    "    @params iou_threshold: float\n",
    "\n",
    "    Returns:\n",
    "    @params accuracy: float\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    iou = torchvision.ops.box_iou(box_pred, box_true).diagonal()\n",
    "    giou = torchvision.ops.generalized_box_iou(box_pred, box_true).diagonal()\n",
    "\n",
    "    return (iou > iou_threshold).float().mean(), iou.mean(), giou.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Gigiate Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"RN50\")\n",
    "model = model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_data():\\n  a = 0\\n  images = []\\n  texts = []\\n\\n  for d in dataloader:\\n    texts.append(d[1][\"caption\"][0])\\n    if a == 4:\\n        break\\n    a += 1\\n  a = 0\\n  for d in dataset:\\n    images.append(d[\\'file_name\\'])\\n    if a == 4:\\n        break\\n    a += 1\\n    \\n  return images, texts\\n\\nimages_fp, texts = get_data()\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_data(images_fp: list[str], texts: list[str]):\n",
    "  # preprocess the images to transform from filenames to images to tensors\n",
    "  images = [preprocess(Image.open(image)) for image in images_fp]\n",
    "\n",
    "  # preprocess the texts to transform from text to tensors\n",
    "  images = torch.tensor(np.stack(images)).cuda()\n",
    "  text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).cuda()\n",
    "\n",
    "  # encode the inputs\n",
    "  with torch.no_grad():\n",
    "    images_z = model.encode_image(images).float()\n",
    "    texts_z = model.encode_text(text_tokens).float()\n",
    "  \n",
    "  return images_z, texts_z\n",
    "\n",
    "def cosine_similarity(images_z: torch.Tensor, texts_z: torch.Tensor):\n",
    "  # normalise the image and the text\n",
    "  images_z /= images_z.norm(dim=-1, keepdim=True)\n",
    "  texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
    "\n",
    "  # evaluate the cosine similarity between the sets of features\n",
    "  similarity = (texts_z @ images_z.T)\n",
    "\n",
    "  return similarity.cpu()\n",
    "\"\"\"\n",
    "def get_data():\n",
    "  a = 0\n",
    "  images = []\n",
    "  texts = []\n",
    "\n",
    "  for d in dataloader:\n",
    "    texts.append(d[1][\"caption\"][0])\n",
    "    if a == 4:\n",
    "        break\n",
    "    a += 1\n",
    "  a = 0\n",
    "  for d in dataset:\n",
    "    images.append(d['file_name'])\n",
    "    if a == 4:\n",
    "        break\n",
    "    a += 1\n",
    "    \n",
    "  return images, texts\n",
    "\n",
    "images_fp, texts = get_data()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images_fp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m images_z, texts_z \u001b[39m=\u001b[39m encode_data(images_fp, texts)\n\u001b[1;32m      2\u001b[0m similarity \u001b[39m=\u001b[39m cosine_similarity(images_z, texts_z)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(similarity)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'images_fp' is not defined"
     ]
    }
   ],
   "source": [
    "images_z, texts_z = encode_data(images_fp, texts)\n",
    "similarity = cosine_similarity(images_z, texts_z)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and models\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# YOLO\n",
    "model_yolo = YOLO(\"yolov8x.pt\")\n",
    "\n",
    "# CLIP\n",
    "clip_model, clip_preprocess = clip.load(\"RN50\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 FineTuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Creating train, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitData(batch_size=64, transform=None, test_batch_size=64):\n",
    "    if not transform:\n",
    "        # convert the PIL images to Tensors\n",
    "        transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "    # load data\n",
    "    full_training_data = RefCOCOGDataset(root_dir=dataset_root, split_type='train')\n",
    "    test_data = RefCOCOGDataset(root_dir=dataset_root, split_type='test')\n",
    "    # create train and validation splits\n",
    "    num_samples = len(full_training_data)\n",
    "    training_samples = int(num_samples * 0.5 + 1)\n",
    "    validation_samples = num_samples - training_samples\n",
    "\n",
    "    training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\n",
    "\n",
    "    # initialize dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Test Step Zero-Shot Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step_zero_shot_clip(net, data_loader, texts_z, device='cuda'):\n",
    "  samples = 0.0\n",
    "  cumulative_accuracy = 0.0\n",
    "\n",
    "  # set the network to evaluation mode\n",
    "  net.eval()\n",
    "\n",
    "  # disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
    "  with torch.no_grad():\n",
    "    # iterate over the test set\n",
    "    for batch, (inputs,targets) in enumerate(data_loader):\n",
    "\n",
    "      print(data.keys())\n",
    "      #print(targets)\n",
    "      # load data into GPU\n",
    "      inputs = inputs.to(device)\n",
    "      targets = targets.to(device)\n",
    "        \n",
    "      # forward pass\n",
    "      # these two lines are different from the \"traditional\" ones\n",
    "      images_z = model.encode_image(inputs).float()\n",
    "      outputs = (100 * images_z @ texts_z.T).softmax(dim=-1)\n",
    "\n",
    "      # fetch prediction and loss value\n",
    "      samples += inputs.shape[0]\n",
    "      _, predicted = outputs.max(1)\n",
    "\n",
    "      # compute accuracy\n",
    "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "  return cumulative_accuracy / samples * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two woman one in black eatting and the other has a white shirt at the desk\n"
     ]
    }
   ],
   "source": [
    "for d in dataset:\n",
    "    print(d['caption'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_batches(dataset, batch_size):\n",
    "    # Extract descriptions from the dataset\n",
    "    descriptions = [data['target'] for data in dataset]\n",
    "    \n",
    "    # Initialize lists to store tokenized textual features\n",
    "    all_texts_z = []\n",
    "\n",
    "    # Tokenize and encode in batches\n",
    "    for batch_start in range(0, len(descriptions), batch_size):\n",
    "        batch_descriptions = descriptions[batch_start:batch_start + batch_size]\n",
    "\n",
    "        # Tokenize batch and move to GPU\n",
    "        text_tokens = clip.tokenize(batch_descriptions).cuda()\n",
    "\n",
    "        # Encode batch and normalize\n",
    "        with torch.no_grad():\n",
    "            texts_z = model.encode_text(text_tokens).float()\n",
    "            texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        all_texts_z.append(texts_z)\n",
    "\n",
    "    # Concatenate and return all batches\n",
    "    return torch.cat(all_texts_z, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"RN50\")\n",
    "model = model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_name = \"cifar10\"\n",
    "\n",
    "batch_size_token = 64\n",
    "\n",
    "_, _, test_loader = SplitData(transform=preprocess)\n",
    "texts_z = tokenize_batches(test,batch_size_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(test_loader[\u001b[39m0\u001b[39;49m])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "print(test_loader[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/VisualGrounding/DeepLearning/DeepBlasphemy/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:127\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m elem_type({key: collate([d[key] \u001b[39mfor\u001b[39;49;00m d \u001b[39min\u001b[39;49;00m batch], collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;49;00m key \u001b[39min\u001b[39;49;00m elem})\n\u001b[1;32m    128\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     \u001b[39m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/VisualGrounding/DeepLearning/DeepBlasphemy/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:127\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m elem_type({key: collate([d[key] \u001b[39mfor\u001b[39;49;00m d \u001b[39min\u001b[39;49;00m batch], collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem})\n\u001b[1;32m    128\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     \u001b[39m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/VisualGrounding/DeepLearning/DeepBlasphemy/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:150\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]\n\u001b[0;32m--> 150\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem_type))\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_accuracy \u001b[39m=\u001b[39m test_step_zero_shot_clip(model, test_loader, texts_z)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTest accuracy \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(test_accuracy))\n",
      "Cell \u001b[0;32mIn[60], line 11\u001b[0m, in \u001b[0;36mtest_step_zero_shot_clip\u001b[0;34m(net, data_loader, texts_z, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39m# disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     10\u001b[0m   \u001b[39m# iterate over the test set\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m   \u001b[39mfor\u001b[39;00m batch, (inputs,targets) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(data_loader):\n\u001b[1;32m     13\u001b[0m     \u001b[39mprint\u001b[39m(data\u001b[39m.\u001b[39mkeys())\n\u001b[1;32m     14\u001b[0m     \u001b[39m#print(targets)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[39m# load data into GPU\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/VisualGrounding/DeepLearning/DeepBlasphemy/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Documents/VisualGrounding/DeepLearning/DeepBlasphemy/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Documents/VisualGrounding/DeepLearning/DeepBlasphemy/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/Documents/VisualGrounding/DeepLearning/DeepBlasphemy/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/Documents/VisualGrounding/DeepLearning/DeepBlasphemy/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:130\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[39mreturn\u001b[39;00m elem_type({key: collate([d[key] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m batch], collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem})\n\u001b[1;32m    128\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         \u001b[39m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m         \u001b[39mreturn\u001b[39;00m {key: collate([d[key] \u001b[39mfor\u001b[39;49;00m d \u001b[39min\u001b[39;49;00m batch], collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;49;00m key \u001b[39min\u001b[39;49;00m elem}\n\u001b[1;32m    131\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(elem, \u001b[39m'\u001b[39m\u001b[39m_fields\u001b[39m\u001b[39m'\u001b[39m):  \u001b[39m# namedtuple\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[39mreturn\u001b[39;00m elem_type(\u001b[39m*\u001b[39m(collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch)))\n",
      "File \u001b[0;32m~/Documents/VisualGrounding/DeepLearning/DeepBlasphemy/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:130\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[39mreturn\u001b[39;00m elem_type({key: collate([d[key] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m batch], collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem})\n\u001b[1;32m    128\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         \u001b[39m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m         \u001b[39mreturn\u001b[39;00m {key: collate([d[key] \u001b[39mfor\u001b[39;49;00m d \u001b[39min\u001b[39;49;00m batch], collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem}\n\u001b[1;32m    131\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(elem, \u001b[39m'\u001b[39m\u001b[39m_fields\u001b[39m\u001b[39m'\u001b[39m):  \u001b[39m# namedtuple\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[39mreturn\u001b[39;00m elem_type(\u001b[39m*\u001b[39m(collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch)))\n",
      "File \u001b[0;32m~/Documents/VisualGrounding/DeepLearning/DeepBlasphemy/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:150\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m             \u001b[39m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[1;32m    148\u001b[0m             \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]\n\u001b[0;32m--> 150\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem_type))\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>"
     ]
    }
   ],
   "source": [
    "test_accuracy = test_step_zero_shot_clip(model, test_loader, texts_z)\n",
    "\n",
    "print(\"Test accuracy {:.2f}\".format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
